---
title: "dada2_micro_fwd"
author: "diana baetscher"
date: "2026-02-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{sh, eval = F}
# preliminaries to get cutadapt goingq
(base) AKCJM140-DB23:cutadapt_mismatches diana.baetscher$ conda activate cutadaptenv

DATA=/Users/diana.baetscher/git-repos/HAPO/data/20260122_HAPOeDNA/rawdata

# below, the first set of () creates an array, containing n elements of the desired trimmed and unique names
NAMELIST=$(ls ${DATA} | sed 's/e*_L001.*//' | uniq)
echo "${NAMELIST}"

```


```{sh, eval=F}
# cutadapt remove primers
# single-end, unsure of which side we're sequencing... begin with the forward primer, which has the `-g` for indicating it is the 5' end, not the 3' end:
for i in ${NAMELIST}; do
	cutadapt --discard-untrimmed -g TACTCCTTGAAAAAGCCCATTGTA -o trimmed/${i}_R1.fastq.gz "$DATA/${i}_L001_R1_001.fastq.gz" >> cutadapt_fwd_out.txt
	done


# then unzip 
pigz -d *gz
```


```{r load-libraries}
library(dada2)
library(tidyverse)
```
I moved all the files that are not completely empty to this new directory.
```{r}
# file location
path <- "../data/20260122_HAPOeDNA/trimmed"

list.files(path)
```

```{r}
fnFs <- sort(list.files(path, pattern = "_R1.fastq", full.names = TRUE))
#fnRs <- sort(list.files(path, pattern = "_R2.fastq", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
```

```{r}
plotQualityProfile(fnFs[1:6])
#plotQualityProfile(fnRs[1:2])
```
Wow, the quality of some of those sequences is bad at ~100 bp.


```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
names(filtFs) <- sample.names
```


I think I just need to process the Fwd reads
```{r}
# trim reads using the quality scores
out <- filterAndTrim(fnFs, filtFs, truncLen = 200,
              maxN=0, truncQ=10, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) 

head(out)

tmp <- out %>%
  as.data.frame() %>%
  filter(reads.out <1) %>%
  rownames_to_column(var = "file") %>%
  select(file) %>%
  mutate(loc = paste0(path,"/",file)) %>%
  select(loc)

empty_files <- tmp$loc

# create a directory for files with no reads
#dir.create("../data/20260122_HAPOeDNA/trimmed/no_reads", showWarnings = FALSE)

# move the files
# file.rename(empty_files,
#             file.path("../data/20260122_HAPOeDNA/trimmed/no_reads", basename(empty_files)))
```
Those filters worked decently for now - keeping a majority of the reads.

I need to remove 34 samples from the rest of the analysis.


Let's move fwd with these for now... and come back if there are other issues.

### Error rates
```{r}
my_list <- as.list(filtFs)
new_list <- my_list[1:3]

# fwd error rates
errF <- learnErrors(new_list, multithread=TRUE)
# plot the erors
p1 <- plotErrors(errF, nominalQ=TRUE)

p1

```



### Sample inference

```{r}
# forwards
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)

```


Make a sequence table
```{r}
seqtab <- makeSequenceTable(dadaFs)
dim(seqtab)

as_data_frame(getSequences(seqtab))
```

```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```
Let's remove the singletons and off-target sequences
```{r}
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 200]
```

Remove chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```



## Export files for taxonomy and samples/ASVs

```{r regseqs-asv-output}
 #make fasta file with ASVs
    asv_seqs=colnames(seqtab.nochim)
    for(i in 1:length(asv_seqs))
    {
        write.table(paste(">ASV",i, sep=""),file="csv_outputs/HAPO_micro_ASV_fwd200bp.csv", append=TRUE, col.names = F, row.names = F, quote=F)
        write.table(paste(asv_seqs[i], sep=""),file="csv_outputs/HAPO_micro_ASV_fwd200bp.csv", append=TRUE, col.names = F, row.names = F, quote=F)
    }
```

That's the input for the FASTA blastn search.

Goal: change ASV headers to numbered ASVs that correspond to those output in the FASTA file.
```{r output-asv-table}
# Make map between brief names and full sequences
briefToSeq <- colnames(seqtab.nochim)
names(briefToSeq) <- paste0("ASV", seq(ncol(seqtab.nochim))) # Seq1, Seq2, ...
# Make new sequence table with brief names
st.brief <- seqtab.nochim
colnames(st.brief) <- names(briefToSeq)

# export the pool seq table with brief names:
write.csv(st.brief, file="csv_outputs/microkit_fwd_sampleTable.csv")
```


